---
title: "Price predictors of used cars"
author: "Bilal Naseem, Franco Doi (30270314), Gautham Chandra Shekariah, Joao Bertti Targino, Romith Bondada"
date: "2025-03-30"
output: 
  bookdown::word_document2:
    toc: true
    number_sections: true
editor_options: 
  markdown: 
    wrap: sentence
header-includes:
  - \usepackage{amsmath}
  - \usepackage{hyperref}
  - \usepackage{graphicx}
  - \usepackage{float} 
  - \usepackage{listings}
  - \usepackage{caption}
  - \usepackage{booktabs}
  - \setcounter{figure}{0}
  - \lstset{breaklines=true, xleftmargin=20pt, basicstyle=\tiny, frame=single}
geometry: margin=1in
fontsize: 11pt
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(mosaic)
library(tidyverse)
library(webshot2)
library(latex2exp)
library(olsrr)
library(bookdown)


library(rmarkdown)
library(officer)

library(corrplot)
library(ggplot2)
library(gridExtra)
library(scales)
library(lubridate)
library(knitr)
library(reshape2)

```

\newpage
**DATA 603 (W25)**  
**Final Report**

**Title**: Price Predictors of Used Cars  

**Group Members**:  

*Full names and UCIDS*

- Bilal Naseem  
- Franco Doi (30270314)  
- Gautham Chandra Shekariah  
- Joao Bertti Targ

\newpage
# INTRODUCTION 

## Motivation

### Context
*Clearly state the context and applied domain(s) of your project.*

The automotive market for pre-owned or used vehicles is dynamic and often unpredictable. As noted in the study "Predicting Car Prices: A Comparative Analysis of Machine Learning Techniques" (Abdulhakeem et al., 2023), the accurate prediction of car prices is crucial for various stakeholders in the automotive industry, including manufacturers, dealers, and consumers.

The used car market presents unique challenges for price prediction due to the numerous factors that influence a vehicle's value beyond just its base specifications. These factors include age, mileage, condition, brand reputation, market demand, and regional economic conditions. Traditional methods of car valuation often fail to capture the complex relationships between these variables.

Machine learning approaches offer a promising solution to this problem by analyzing patterns in historical data to generate accurate price predictions. The referenced study compares several machine learning algorithms including Linear Regression, Random Forest, and XGBoost to determine which provides the most accurate predictions for used car prices.


### Problem
*What problem are we going to address and propose a data/visual analytics solution to?*

This project aims to develop a predictive model that estimates the sale price of a pre-owned vehicle based on a variety of numerical and categorical features. The motivation stems from a personal and practical interest: as individuals who may buy or sell used cars, we want to ensure we are not overpaying or underselling. A reliable prediction model could help users make more informed financial decisions in the used car market. 

### Challenges
*Why is the problem hard? Provide examples*

Predicting the price of a used vehicle involves several challenges. First, the data may contain missing values, inconsistent formats, or outliers that need to be cleaned and handled appropriately. Second, the relationship between features (such as brand, age, mileage, and condition) and price are often nonlinear and complex. Third, combining both categorical and numerical variables in a model requires careful preprocessing and feature engineering. Finally, ensuring the model generalizes well to unseen data demands rigorous validation and tuning.

## Objectives

### Overview
*What is the overall intent of the project?*

The objective of this project is to build a predictive model that can accurately estimate the sale price of a pre-owned/used car. This model will leverage a combination of numerical and categorical variables to understand the factors that influence pricing in the used car market. By applying data analysis and machine learning techniques, we aim to create a practical tool for evaluating whether a vehicle is fairly priced—either for personal use or broader market insight.

### Goals & Research Questions
*Clearly state each of your data/visual analytics goals and research questions.*

#### Goals

1. To analyze the key variables that affect the sale price of used cars.
2. To preprocess and explore the dataset for patterns, correlations, and insights.
3. To build, evaluate, and compare predictive models that estimate car prices based on input features.
4. To create a tool that can help users make more informed decisions when buying or selling a used vehicle.

#### Research Questions

1. What are the most influential features in determining a used car’s sale price?
2. How accurately can we predict the price using machine learning algorithms?
3. Are there noticeable trends or patterns in pricing based on make, model, age, or mileage?
4. Can the model help identify potentially overpriced or underpriced vehicles?













\newpage
# METHODOLOGY  

## Data
*Describe the data set(s) related to your project, source(s), how data was collected, number of attributes and a brief description of each attribute.*

### Data Source
The dataset used in this project is the "Used Car Price Prediction Dataset" obtained from Kaggle (https://www.kaggle.com/datasets/taeefnajib/used-car-price-prediction-dataset). This dataset contains information about used cars listed for sale, including various attributes that potentially influence the selling price.

### Data Collection

The dataset is available online and is licensed under Attribution 4.0 International, which allows us to use for academic purposes. 

The data was likely collected from online car listings or dealership inventory, though the specific collection methodology isn't explicitly stated in the Kaggle source. Without additional metadata, it's difficult to determine if the sampling was random or if there are geographic or temporal biases in the collection.

### Dataset Size
The dataset consists of 6,019 observations (rows), each representing a specific used car listing. It contains 13 variables (columns) capturing various car attributes that might influence pricing.

### Attributes Description
Key features in the dataset include:

- Vehicle characteristics (make, model, year, mileage)
- Physical attributes (engine capacity, fuel type, transmission type)
- Market factors (listing price)

Detailed explanation of each attribute is below:

-	**Price**: The listed selling price of the used car (in currency units). This is the target variable we aim to predict.
-	**Brand**: The manufacturer or make of the car (e.g., Toyota, Honda, Ford, BMW, Mercedes-Benz). This is a categorical variable representing different car manufacturers.
-	**Model**: The specific model of the car offered by the manufacturer. This is a categorical variable with numerous unique values depending on the brand.
-	**Year**: The manufacturing year of the vehicle. This numerical variable indicates the car's age, which typically has a strong correlation with its price.
-	**Mileage**: The total distance the car has been driven in miles. This numerical variable is a key indicator of vehicle wear and significantly impacts resale value.
-	**Fuel Type**: The type of fuel the car uses (e.g., Gasoline, Diesel, Electric, Hybrid). This categorical variable can significantly influence pricing due to differences in fuel economy and operational costs.
-	**Engine Capacity**: The engine’s specifications including size, technology, number of cylinders, generally correlates with the car's power and fuel consumption.
-	**Transmission Type**: The type of transmission system in the car (e.g., Automatic, Manual, CVT, Semi-automatic). This categorical variable affects both driving experience and maintenance costs.
- **Exterior Color**: Exterior color of the car. The exterior color significantly influences a vehicle's marketability and resale value.
- **Interior Color**: Interior color of the car. Interior color affects both buyer perception and practical considerations.
- **Accident**: Accident history of the car. A vehicle's accident history can dramatically impact its value and desirability.

The dataset provides a comprehensive representation of the used car market with attributes covering the most significant factors that influence car pricing. These variables include both technical specifications (engine capacity, mileage) and market segmentation factors (brand, model), allowing for a thorough analysis of price determinants.

## Approach
*What approach are you using for data/visual analytics solution? Why do you think it will work well?*

For our used car price prediction project, we are implementing a comprehensive data and visual analytics solution that combines exploratory data analysis (EDA), statistical modeling, and rigorous validation techniques. Our approach consists of several integrated components:


### Exploratory Data Analysis with Visualizations

We begin with an extensive EDA that uses a variety of visualizations to understand the dataset's characteristics:

a)	**Distribution analyses** of continuous variables (price, mileage, year) using histograms and density plots 
b)	**Relationship visualizations** through scatter plots, box plots, and correlation matrices 
c)	**Category-based comparisons** using bar charts and grouped box plots to assess how categorical variables like brand, model, and fuel type affect prices
d)	**Multi-dimensional analyses** including faceted plots and color-coded visualizations to reveal complex interactions between variables


### Modeling with Multiple Linear Regression

For prediction, we employ a multiple linear regression model with several enhancements:

a)	**Interaction terms** between significant variables to account for conditional effects 
b)	**Polynomial transformations** of key numerical predictors to capture non-linear relationships 
c)	**Feature engineering** including derived variables like car age and miles-per-year 
d)	**Comprehensive model selection** using techniques like best subset selection, adjusted R-squared, AIC, and BIC criteria


### Statistical Validation and Diagnostics

We implement a rigorous validation framework:

a)	**Assumption testing** for linearity, independence, homoscedasticity, and normality 
b)	**Multicollinearity detection** using Variance Inflation Factors (VIF) 
c)	**Influential observation analysis** using Cook's distance and DFBETAS 
d)	**Cross-validation** with train/test splits to confirm model generalizability 
e)	**Performance metrics** including RMSE and R-squared

### Results Visualization and Interpretation
Our final analytics layer includes:

a)	**Coefficient visualization** to show the impact of each predictor 
b)	**Actual vs. predicted** plots to illustrate model accuracy 
c)	**Residual analysis visualizations** to identify potential improvements 
d)	**Interactive visualizations** that allow for dynamic exploration of model predictions

### Why this works
Our integrated approach is particularly well-suited for the used car price prediction problem for several reasons:

1)	**Addresses Domain-Specific Challenges**

    The used car market has well-known non-linear aspects (e.g., depreciation curves) and complex interactions (e.g., how premium brands depreciate differently) that our approach specifically targets through polynomial terms and interaction effects.
  
2)	**Balances Complexity and Interpretability**
  
    While machine learning algorithms like random forests might achieve high accuracy, linear regression with carefully selected enhancements provides comparable performance while maintaining interpretability—a crucial factor when the goal is not just prediction but understanding what drives car prices.
  
3)	**Provides Rigorous Statistical Foundation**
  
    Our statistical validation ensures that the model meets all necessary assumptions, preventing misleading results and ensuring the reliability of predictions across different market segments.
  
4)	**Leverages Visual Analytics for Insight Generation**
  
    By integrating visualization throughout the analysis pipeline—from initial exploration to final model evaluation—we facilitate deeper insights about the used car market that purely algorithmic approaches might miss.
  
5)	**Enables Practical Application**

    The final model delivers actionable insights for various stakeholders:

    +	Buyers can understand fair market values based on car attributes
    +	Sellers can set competitive yet profitable prices
    +	Dealerships can optimize inventory pricing strategies
    +	Market analysts can identify trends and anomalies in the used car market

By combining robust statistical modeling with comprehensive visual analytics, our approach not only predicts prices with high accuracy but also provides valuable insights into the factors that drive the used car market.

## Workflow
*What steps (workflow task list) are required? Which of these steps is particularly hard? What to do if the hard steps don't work out*

The workflow for building a robust used car price prediction model is as follows:

1. **Exploratory Data Analysis (EDA)**

    The first step involves understanding the structure, distribution, and relationships within the dataset. This includes identifying missing values, outliers, data types, and potential data quality issues. EDA also helps form initial hypotheses and informs feature engineering strategies.
    
2. **Data Cleaning and Preprocessing**

    This includes handling missing values, correcting inconsistent entries, and converting categorical features to a usable format (e.g., one-hot encoding or label encoding). Normalization or scaling of numerical features may also be applied here.

3. **Feature Selection and Collinearity Check**

    Highly correlated features can lead to multicollinearity, which may negatively impact some models. Identifying and removing redundant features helps improve model performance and interpretability.

4. **Modeling**

    Build a linear regression model. Include polynomial terms or interactions if they improve performance. 

5. **Evaluation**

    Use metrics such as MAE, RMSE, and R² on validation data to assess model accuracy. Validate with a train/test split or cross-validation.

6. **Iterative Tuning**

    Based on validation results, revisit earlier steps—adjust feature engineering, try new models, or fine-tune hyperparameters. This iterative loop continues until a satisfactory model is achieved.
    
As industry standard value $\alpha$ will be set as $0.05$ throughout the project.

## Challenges and Potential issues

Given the nature of used car listings, potential sampling biases might include:

* Over-representation of popular car models
* Limited geographic coverage if data was collected from specific regions
* Possible exclusion of private sales if only dealership data was used

The dataset appears designed primarily for developing machine learning models to predict used car prices based on vehicle attributes.


## Contributions

*Briefly describe the group members’ workload distribution and responsibilities.*








```{r}
# Function to create bar plots for categorical variables
plot_categorical_var <- function(data, var_name, figure_no, max_categories = 25) {
  # Increment figure number
  figure_no <- figure_no + 1
  
  # Get top N categories if there are too many
  if(length(unique(data[[var_name]])) > max_categories) {
    top_cats <- names(sort(table(data[[var_name]]), decreasing = TRUE)[1:max_categories])
    plot_data <- data %>%
      mutate(temp_var = ifelse(.data[[var_name]] %in% top_cats, 
                              as.character(.data[[var_name]]), 
                              "Other")) %>%
      count(temp_var) %>%
      rename(!!var_name := temp_var)
    
    # Create caption for plots with "Other" category
    # caption_text <- paste("Figure", figure_no, ": Showing top", max_categories, 
    #                      "categories of ",var_name,". All remaining categories grouped as 'Other'.")
  } else {
    plot_data <- data %>% 
      count(.data[[var_name]])
    
    # Create caption for plots with all categories shown
    caption_text <- paste("Figure", figure_no, ": Distribution of all", 
                         length(unique(data[[var_name]])), "categories.")
  }
  
  # Create the plot using tidy evaluation
  p <- ggplot(plot_data, aes(x = .data[[var_name]], y = n)) +
    geom_bar(stat = "identity", fill = "seagreen", alpha = 0.7) +
    labs(title = paste("Count of Cars by", var_name),
         x = var_name,
         y = "Count",
        ) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5),
          axis.text.x = element_text(angle = 45, hjust = 1),
          plot.caption = element_text(hjust = 0.5, face = "italic"))
  
  # Return both the plot and the updated figure number
  return(list(plot = p, figure_no = figure_no))
}
```









\newpage
# MAIN RESULTS OF THE ANALYSIS

## Results

*What do my results indicate? Do you have any unexpected results? Please elaborate.*

### Data cleaning & transformation

The numerical features price and milage were string like “\$10,300” for price and “5,1000 mi” for milage.
The characters “\$” and “,” were removed for price and “,”, “ “ and “mi” were removed for milage prior to converting them to numerical features.
Column milage was renamed to mileage for easier understanding.


```{r}
# 1. Data Loading and Initial Exploration
# ---------------------------------------

zip_url <- "https://www.kaggle.com/api/v1/datasets/download/taeefnajib/used-car-price-prediction-dataset"
# Create a temporary file to download the ZIP
temp_file <- tempfile(fileext = ".zip")

# Download the file
download.file(zip_url, temp_file, mode = "wb")

# List files inside the ZIP
csv_file_name <- unzip(temp_file, list = TRUE)$Name[1]  # choose appropriate file if more than one

used_cars_data <- read.csv(unz(temp_file, csv_file_name))
```

```{r}
# remove string and convert string to number
used_cars_data$price <- gsub("[$,]", "", used_cars_data$price)
used_cars_data$price <- as.integer(used_cars_data$price)

# remove string and convert string to number
used_cars_data$milage <- gsub("[, mi]", "", used_cars_data$milage)
used_cars_data$milage <- as.integer(used_cars_data$milage)

#renaming column
names(used_cars_data)[names(used_cars_data) == "milage"] <- "mileage"

# Replace missing 'fuel_type' values with "electric"
used_cars_data$fuel_type[is.na(used_cars_data$fuel_type)] <- "electric"
```

Categorical attributes are converted into type factor to avoid wrong interpretation by R
```{r}
# used_cars_data$brand <- as.factor(used_cars_data$brand)
used_cars_data$model <- as.factor(used_cars_data$model)
used_cars_data$fuel_type <- as.factor(used_cars_data$fuel_type)
used_cars_data$engine <- as.factor(used_cars_data$engine)
used_cars_data$transmission <- as.factor(used_cars_data$transmission)
used_cars_data$ext_col <- as.factor(used_cars_data$ext_col)
used_cars_data$int_col <- as.factor(used_cars_data$int_col)
used_cars_data$accident <- as.factor(used_cars_data$accident)
used_cars_data$clean_title <- as.factor(used_cars_data$clean_title)
```

### EDA
#### Summary
After data cleaning the dataset has 4009 rows and 12 attributes, and corresponding data types.
```{r}
# Display basic information about the dataset
str(used_cars_data)
```

Below is the summary of the dataset
```{r}
summary(used_cars_data)
```

There is no missing values nor duplication.
```{r}
# Check for missing values
missing_values <- colSums(is.na(used_cars_data))
cat("\nMissing values per column:\n")
print(missing_values[missing_values > 0])
```
```{r}
# Check for duplicates
cat("\nNumber of duplicate rows:", sum(duplicated(used_cars_data)), "\n")
```

#### Feature Analysis
##### Price distribution

```{r}
figure_no <- 1
ggplot(used_cars_data, aes(x = price)) +
  geom_histogram(bins = 50, fill = "steelblue", alpha = 0.7) +
  scale_x_continuous(labels = scales::comma) +
  labs(title = "Distribution of Car Prices",
       x = "Price ($)",
       y = "Count",
       caption = paste("Figure", figure_no,": Histogram showing the distribution of used car prices with 50 bins.")) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        plot.caption = element_text(hjust = 0.5, face = "italic"))
```

The plot above shows that most car prices are less than 1,000,000, and clearly have outliers, which makes the analysis of distribution hard to visualize.

To visualize the distribution with less impact of the outliers, the car prices are transformed by log:

```{r}
figure_no <- figure_no + 1
ggplot(used_cars_data, aes(x = log(price))) +
  geom_histogram(bins = 50, fill = "steelblue", alpha = 0.7) +
  labs(title = "Distribution of Log-Transformed Car Prices",
       x = "Log(Price)",
       y = "Count",
       caption = paste("Figure", figure_no, ": Log transformation applied to price values to better visualize the distribution pattern.")) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        plot.caption = element_text(hjust = 0.5, face = "italic"))
```

The figure above shows that the distribution is slightly skewed to the right, probably caused by the outliers.

##### Manufacturing year distribution

```{r}
figure_no <- figure_no + 1
ggplot(used_cars_data, aes(x = model_year)) +
  geom_histogram(bins = 50, fill = "steelblue", alpha = 0.7) +
  scale_x_continuous(labels = scales::comma) +
  labs(title = "Distribution of Car's manufacturing year",
       x = "Manufacturing year",
       y = "Count",
       caption = paste("Figure",figure_no,": Histogram showing the distribution of used car manufacturing year with 50 bins.")) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        plot.caption = element_text(hjust = 0.5, face = "italic"))
```


```{r}
used_cars_data$car_age <- 2024 - used_cars_data$model_year
```

```{r}
ggplot(used_cars_data, aes(x = car_age)) +
  geom_histogram(bins = 50, fill = "steelblue", alpha = 0.7) +
  scale_x_continuous(labels = scales::comma) +
  labs(title = "Distribution of Car's manufacturing year",
       x = "Manufacturing year",
       y = "Count") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        plot.caption = element_text(hjust = 0.5, face = "italic"))
```


The figure above shows distribution for the model year on the left and the mileage on the right.

The model year is skewed to the left, where most of the cars are newer.

##### Mileage distribution
```{r}
figure_no <-0
ggplot(used_cars_data, aes(x = mileage)) +
  geom_histogram(bins = 50, fill = "steelblue", alpha = 0.7) +
  scale_x_continuous(labels = scales::comma) +
  labs(title = "Distribution of Car's mileage",
       x = "Mileage",
       y = "Count", 
       caption = paste("Figure",figure_no,": Histogram showing the distribution of used car mileage with 50 bins.")) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        plot.caption = element_text(hjust = 0.5, face = "italic"))
```

```{r}
# Calculating expected mileage
used_cars_data$expected_mileage <- used_cars_data$car_age * 13500

# Defining bins and labels
bins <- c(0, 0.25, 0.5, 0.75, 1, Inf)
labels <- c('verylow', 'low', 'avg', 'high', 'above_expected')

# Assigning mileage status using cut
used_cars_data$mileage_status <- cut(
  used_cars_data$mileage / used_cars_data$expected_mileage,
  breaks = bins,
  labels = labels,
  include.lowest = TRUE
)
```

```{r}
result <- plot_categorical_var(used_cars_data, "mileage_status", figure_no)
result$plot
```



The Mileage is skewed to the right, where most of the cars were driven less.

##### Brand type Distribution

```{r}
result <- plot_categorical_var(used_cars_data, "brand", figure_no)
result$plot
```

Visually the distribution of brands is not uniform, which indicates unbalanced data regarding brand and may bias the regression.

Brands has no invalid or missing value.

Since every car has a brand and a model, therefore brand and model are highly correlated and will violate the multicollinearity assumption. 

Because luxury brands affects perception of value of a car, a new feature **luxury_brand** will be engineered from brand with following rules:

1. If Acura, Alfa, Aston, Audi, Bentley, BMW, Bugatti, Ferrari, INFINITI, Jaguar, Lamborghini, Land, Lexus, Maserati, McLaren, Mercedes-Benz, Porsche, Rolls-Royce, Tesla, Volvo = 1
2. All others = 0

```{r}
length(unique(used_cars_data$brand))
sort(unique(used_cars_data$brand))
```

```{r}
luxury_brands <- c("Acura", "Alfa", "Aston", "Audi", "Bentley", "BMW", "Bugatti",
                   "Ferrari", "INFINITI", "Jaguar", "Lamborghini", "Land", "Lexus", "Maserati",
                   "McLaren", "Mercedes-Benz", "Porsche", "Rolls-Royce", "Tesla", "Volvo")

# Create the new feature
used_cars_data$luxury_brand <- ifelse(used_cars_data$brand %in% luxury_brands, 1, 0)
used_cars_data$luxury_brand <- as.factor(used_cars_data$luxury_brand)
```



```{r}
result <- plot_categorical_var(used_cars_data, "luxury_brand", figure_no)
result$plot
```

There is a balanced distribution between luxury and non luxury brand.


##### Model distribution
```{r}
result <- plot_categorical_var(used_cars_data, "model", figure_no)
result$plot
```

Visually the distribution of car models looks uniform, with count of other models being the highest.

The models has no invalid value.

```{r}
length(unique(used_cars_data$model))
```


##### Fuel type distribution

```{r}
result <- plot_categorical_var(used_cars_data, "fuel_type", figure_no)
result$plot
```

```{r}
# Replace values in the fuel_type column
used_cars_data$fuel_type <- recode(
  used_cars_data$fuel_type,
  'Hybrid' = 'Hybrid',
  'Plug-In Hybrid' = 'Hybrid',
  'E85 Flex Fuel' = 'Other',
  'not supported' = 'Other',
  '–' = 'Other',
  'electric' = 'Electric'
)
# Replace empty strings in fuel_type with "Other"
used_cars_data$fuel_type[used_cars_data$fuel_type == ""] <- "Other"
```

```{r}
unique(used_cars_data$fuel_type)
```


```{r}
result <- plot_categorical_var(used_cars_data, "fuel_type", figure_no)
result$plot
```

Gasoline is the most popular fuel type, and there are two unexpected fuel type:
1. Dash (-) : it is an error entry or data was unavailable, in total are 45 rows.
2. Empty string: it is for electric cars, in total are 170 rows.
3. "not supported": it is for electric cars, in total are 2 rows.

For the electric cars, the fuel type is set to "Electric".


For the rows with "Dash", it is observed that engine is also a "Dash", making it impossible to sanitize and will be left for now.


##### Engine type distribution
```{r}
result <- plot_categorical_var(used_cars_data, "engine", figure_no)
result$plot
```

Visually the distribution of car engine looks uniform, with count of other engine types being the highest.

The engine attribute has '-' which are missing values, on the same rows as missing values for fuel type.

This attribute is very descriptive and does not have a clear standard like '3.0 Liter' and '3.0L' and might be hard to be used for the final model.

A new feature is engineered called engine_size that extracts 

```{r}
# Use regexec to extract the decimal engine size (like 1.6) before the 'L'
matches <- regexec("([0-9]+\\.[0-9]+)[ ]{0,1}L", used_cars_data$engine)

# Extract the matched group (i.e., the number) or NA if no match
used_cars_data$engine_size <- sapply(
  regmatches(used_cars_data$engine, matches),
  function(x) if (length(x) >= 2) x[2] else NA
)
used_cars_data$engine_size <- as.numeric(used_cars_data$engine_size)
```

It resulted in `r nrow(used_cars_data[is.na(used_cars_data$engine_size), ])` rows without engine size and has `r length(unique(used_cars_data$engine_size))` unique engine size.

```{r}
figure_no <- figure_no + 1
ggplot(used_cars_data, aes(x = engine_size)) +
  geom_histogram(bins = 50, fill = "steelblue", alpha = 0.7) +
  scale_x_continuous(labels = scales::comma) +
  labs(title = "Distribution of Car's engine size",
       x = "Engine Size",
       y = "Count", 
       caption = paste("Figure",figure_no,": Histogram showing the distribution of used car engine size with 50 bins.")) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        plot.caption = element_text(hjust = 0.5, face = "italic"))
```


Most of engine size are on the smaller side, which is most common in the market.



##### Transmission type distribution
```{r}
figure_no <- figure_no + 1
result <- plot_categorical_var(used_cars_data, "transmission", figure_no)
result$plot
```

Visually the distribution of car transmission are mostly A/T, which seems to be short of Automatic Transmission.

The engine attribute has '-' which are missing values for 4 rows.

This attribute is very descriptive and does not have a clear standard like '2-Speed A/T' and '2-Speed Automatic' and might be hard to be used for the final model.

Based on transmission, a new feature is engineered with named *automatic_transmission* which contains 1 if transmission is automatic and 0 otherwise.

```{r}
# Assuming your data frame is named 'used_cars_data'
used_cars_data$automatic_transmission <- ifelse(grepl("A/T", used_cars_data$transmission), 1, ifelse(grepl("Automatic", used_cars_data$transmission), 1, ifelse(grepl("CVT", used_cars_data$transmission), 1, ifelse(grepl("–", used_cars_data$transmission), 1, ifelse(grepl("Transmission w/Dual Shift Mode", used_cars_data$transmission), 1, ifelse(grepl("Auto", used_cars_data$transmission), 1, ifelse(grepl("Overdrive ", used_cars_data$transmission), 1, ifelse(grepl("AT", used_cars_data$transmission), 1, ifelse(grepl("6 Speed At", used_cars_data$transmission), 1, 0)))))))))

used_cars_data$automatic_transmission <- as.factor(used_cars_data$automatic_transmission)
```

```{r}
figure_no <- figure_no + 1
result <- plot_categorical_var(used_cars_data, "automatic_transmission", figure_no)
result$plot
```

There is massively more automatic transmission that manual transmission, as today's society.

##### Exterior color distribution
```{r}
figure_no <- figure_no + 1
result <- plot_categorical_var(used_cars_data, "ext_col", figure_no)
result$plot
```

```{r}
categorize_color <- function(color_col, df) {
  categorize <- function(color) {
    color <- tolower(color) # Convert to lowercase
    if (is.na(color)) {
      return("Other")
    } else if (grepl("black", color)) {
      return("Black")
    } else if (grepl("gray|grey", color)) {
      return("Gray")
    } else if (grepl("silver", color)) {
      return("Silver")
    } else if (grepl("white", color)) {
      return("White")
    } else if (grepl("blue", color)) {
      return("Blue")
    } else if (grepl("red", color)) {
      return("Red")
    } else {
      return("Other")
    }
  }

  # Apply categorize function to the specified column
  df$external_color <- sapply(df[[color_col]], categorize)
  return(df)
}
```

```{r}
categorize_color_int <- function(color_col, df) {
  categorize <- function(color) {
    if (is.na(color)) {
      return("Other")
    }
    
    color <- tolower(color) # Convert to lowercase
    
    if (grepl("black|charcoal|graphite|ebony", color)) {
      return("Black")
    } else if (grepl("beige|brown", color)) {
      return("Beige")
    } else if (grepl("gray", color)) {
      return("Gray")
    } else if (grepl("-", color)) {
      return("Other_NA")
    } else {
      return("Other_Colored")
    }
  }
  
  # Apply the 'categorize' function to the column
  df$internal_color <- sapply(df[[color_col]], categorize)
  return(df)
}
```

```{r}
metallic_color <- function(color_col, df) {
  categorize <- function(color) {
    # Convert to lowercase and handle missing values
    if (is.na(color)) {
      return("Other")
    }
    color <- tolower(color)
    
    # Check for specific keywords
    if (grepl("metallic|pearl|crystal", color)) {
      return("Yes")
    } else {
      return("No")
    }
  }
  
  # Apply the categorize function to the column
  df$metallic_color <- sapply(df[[color_col]], categorize)
  return(df)
}
```

```{r}
used_cars_data <- categorize_color('ext_col',used_cars_data)
used_cars_data<-metallic_color('ext_col',used_cars_data)
```

```{r}
# Frequency table
freq_table <- table(used_cars_data$external_color)

# View frequencies and proportions
freq_table
prop_table <- prop.table(freq_table)
prop_table

# Identify rare categories (e.g., less than 5% of the total)
rare_categories <- names(freq_table[freq_table < (0.05 * nrow(used_cars_data))])
rare_categories
```





Visually the distribution of car exterior color have few colors that are ver common.

The engine attribute has '-' which are missing values for 15 rows. This information would be easily obtained by inspecting the vehicle and updating the data but is not possible in this dataset.

This attribute has typo like 'Blu', and is very descriptive and does not have a clear standard like 'Blue' and 'BLUE', or 'C / C' and might be hard to be used for the final model.


##### Interior color distribution
```{r}
figure_no <- figure_no + 1
result <- plot_categorical_var(used_cars_data, "int_col", figure_no)
result$plot
```

```{r}
used_cars_data<-categorize_color_int('int_col',used_cars_data)
```

```{r}
figure_no <- figure_no + 1
result <- plot_categorical_var(used_cars_data, "internal_color", figure_no)
result$plot
```


Visually the distribution of car interior color have few colors that are ver common.

The engine attribute has '-' which are missing values for 133 rows. This information would be easily obtained by inspecting the vehicle and updating the data but is not possible in this dataset.

This attribute has typo like 'Blk', and is very descriptive and does not have a clear standard like 'Black' and 'BLACK', or 'Sport' and might be hard to be used for the final model.


##### Accident history distribution
```{r}
figure_no <- figure_no + 1
result <- plot_categorical_var(used_cars_data, "accident", figure_no)
result$plot
```

```{r}
# Create the 'accidents' column: Assign "No" if 'accident' is "None reported", otherwise "Yes"
used_cars_data$accidents <- ifelse(
  used_cars_data$accident == "None reported",
  "No",
  "Yes"
)

# Replace NA values in 'accidents' with "No"
used_cars_data$accidents[is.na(used_cars_data$accidents)] <- "No"
```

```{r}
figure_no <- figure_no + 1
result <- plot_categorical_var(used_cars_data, "accidents", figure_no)
result$plot
```


Visually the distribution of car accident history have no accident reported.

There are 133 missing rows with missing value. Since is not trivial to get a car's accident history, there is a chance that these rows will be dropped or the column will be dropped. There is high chance that this column is higly correlated with accident attribute.

##### Clean title distribution
```{r}
figure_no <- figure_no + 1
result <- plot_categorical_var(used_cars_data, "clean_title", figure_no)
result$plot
```

Visually the distribution of car's clean title have "Yes" as clean title reported.

There are 596 missing rows with missing value. 

Since a car only has clean title if has no accident, clean_title and accident are highly and will violated multicollinearity assumption. Since clean_title has more missing values than accident, this column will be dropped.


```{r}
figure_no <- figure_no + 1
pairs(~price+model_year+mileage+engine_size, data=used_cars_data, panel = panel.smooth)
title(main = "Pairs Plot of Used Cars Data", sub = "Price, Model Year, Mileage, Engine Size")

```

The plot above shows no obvious higher degree between price and numerical independent variables.

The plot also shows no linear correlation between the numerical independent variables.


```{r}

```



```{r}
df_dummies_brand <- model.matrix(~ brand - 1, data = used_cars_data)
df_dummies_luxury_brand <- model.matrix(~ luxury_brand - 1, data = used_cars_data)  
df_dummies_model <- model.matrix(~ model - 1, data = used_cars_data)  
df_dummies_fuel_type <- model.matrix(~ fuel_type - 1, data = used_cars_data)  
df_dummies_engine <- model.matrix(~ engine - 1, data = used_cars_data)  
df_dummies_transmission <- model.matrix(~ transmission - 1, data = used_cars_data)  
df_dummies_ext_color <- model.matrix(~ ext_col - 1, data = used_cars_data)  
df_dummies_int_color <- model.matrix(~ int_col - 1, data = used_cars_data)
df_dummies_accident <- model.matrix(~ accident - 1, data = used_cars_data)
df_dummies_clean_title <- model.matrix(~ clean_title - 1, data = used_cars_data)
df_dummies_automatic_transmission <- model.matrix(~ automatic_transmission - 1, data = used_cars_data)
# Combine the dummy variables with the original numerical features (mileage and model_year)
# df_full <- cbind(df_dummies_brand, df_dummies_model, df_dummies_fuel_type, df_dummies_engine, df_dummies_transmission, df_dummies_ext_color, df_dummies_int_color, df_dummies_accident, df_dummies_clean_title, used_cars_data[, c('mileage', 'model_year','price')])
df_full <- cbind(df_dummies_luxury_brand, df_dummies_model, df_dummies_fuel_type, df_dummies_engine, df_dummies_automatic_transmission, df_dummies_accident,  used_cars_data[, c('mileage', 'model_year','price')])
```


```{r}
# Perform PCA on the combined dummy variables and numerical features
pca_result <- prcomp(df_full, center = TRUE, scale. = TRUE)

# Calculate explained variance and determine number of components explaining 90% of variance
explained_variance <- pca_result$sdev^2 / sum(pca_result$sdev^2)
cumulative_variance <- cumsum(explained_variance)

# Set the threshold for the variance to keep, e.g., 90%
threshold <- 0.90
n_components <- which(cumulative_variance >= threshold)[1]

# Select the first n_components from the PCA result
selected_components <- pca_result$x[, 1:n_components]

# Combine the selected components with the target variable 'sales'
df_pca <- data.frame(selected_components, price = used_cars_data$price)

# Fit a linear regression model using the selected components
lm_pca <- lm(price ~ ., data = df_pca)

# Summarize the model
lm_pca_summary <- summary(lm_pca)
```


```{r}
library(car)
options(max.print = 1000000000)
writeLines(capture.output(vif(lm_pca)), "pca_vif.txt")
```

The output of pca_vif.txt shows that VIF for all predictors is 1, so there is no multi collinearity issue.


The code below retrieves only statistically significant predictors after doing individual t-test, with $\alpha = 0.05$
```{r}
coef_table <- lm_pca_summary$coefficients
pca_significant_predictors <- rownames(coef_table)[coef_table[, 4] < 0.05]
```

There are `r length(print(pca_significant_predictors))` significant predictors, listed below.
```{r}
pca_significant_predictors
```

A new dataframe with only significant predictors is derived from pca's dataframe
```{r}
# Create a new data frame with only the significant predictors
# Assuming df contains the original dataset
pca_sign_no_intercept <- setdiff(pca_significant_predictors, "(Intercept)")

pca_significant_df <- df_pca[, pca_sign_no_intercept]

# Add the dependent variable (response) to the new dataframe if needed
pca_significant_df <- cbind(pca_significant_df, price=df_pca$price)  
```




```{r}
# Create the new formula using significant predictors (excluding intercept)
pca_significant_formula <- as.formula(paste("price ~", paste(pca_significant_predictors[pca_significant_predictors != "(Intercept)"], collapse = " + ")))

# Fit the new linear model using the significant predictors only
lm_pca_significant <- lm(pca_significant_formula, data = pca_significant_df)
lm_pca_significant_summary <- summary(lm_pca_significant)
lm_pca_significant_summary
```

```{r}
stepmod=ols_step_both_p(lm_pca_significant,p_enter = 0.05, p_remove = 0.05, details=FALSE)
stepmod_summary <- summary(stepmod$model)
```

After doing ols_step_both_p all predictors are significant.

The best additive model is
$$
\begin{aligned}
\widehat{price} = &`r round(stepmod_summary$coefficients[1],4)` `r round(stepmod_summary$coefficients[2],4)`X_{PC11} `r round(stepmod_summary$coefficients[3],4)`X_{PC12} `r round(stepmod_summary$coefficients[4],4)`X_{PC1}\\
                  &`r round(stepmod_summary$coefficients[5],4)`X_{PC10} `r round(stepmod_summary$coefficients[6],4)`X_{PC8} `r round(stepmod_summary$coefficients[7],4)`X_{PC9} `r round(stepmod_summary$coefficients[8],4)`X_{PC3}\\
                  &`r round(stepmod_summary$coefficients[9],4)`X_{PC2} `r round(stepmod_summary$coefficients[10],4)`X_{PC1044} `r round(stepmod_summary$coefficients[11],4)`X_{PC5} `r round(stepmod_summary$coefficients[12],4)`X_{PC7}\\
                  &`r round(stepmod_summary$coefficients[13],4)`X_{PC1043} `r round(stepmod_summary$coefficients[14],4)`X_{PC13} `r round(stepmod_summary$coefficients[15],4)`X_{PC4} `r round(stepmod_summary$coefficients[16],4)`X_{PC1041}\\
                  &`r round(stepmod_summary$coefficients[17],4)`X_{PC6} `r round(stepmod_summary$coefficients[18],4)`X_{PC782} `r round(stepmod_summary$coefficients[19],4)`X_{PC1042} `r round(stepmod_summary$coefficients[20],4)`X_{PC804}\\
                  &`r round(stepmod_summary$coefficients[21],4)`X_{PC787} `r round(stepmod_summary$coefficients[22],4)`X_{PC864} `r round(stepmod_summary$coefficients[23],4)`X_{PC786}
\end{aligned}
$$

```{r}
# Extract coefficient names
coef_names <- rownames(stepmod_summary$coefficients)

# Remove intercept if present
coef_names <- coef_names[coef_names != "(Intercept)"]

# 2. Subset your existing data frame by these predictor names
pca_additive_final_df <- pca_significant_df[, coef_names, drop = FALSE]
pca_additive_final_df <- cbind(pca_additive_final_df, price=df_pca$price)  


# Create formula string (assuming Y is your response variable)
formula_string <- paste("price ~", paste(coef_names, collapse = " + "))


# Convert to formula
pca_additive_final <- as.formula(formula_string)
pca_additive_final_lm <- lm(pca_additive_final, data=pca_additive_final_df)
```



```{r}
library(car)
options(max.print = 1000000000)
lm_pca_significant_vif <- vif(pca_additive_final_lm)
lm_pca_significant_vif
```

There is no multi collinearity among predictors

### The interactive model

```{r}
find_significant_interactions <- function(coefficients) {
  coef_matrix <- coefficients
  # Convert to data frame for easier filtering
    coef_df <- data.frame(
      Term = rownames(coef_matrix),
      Estimate = coef_matrix[, 1],
      StdError = coef_matrix[, 2],
      tValue = coef_matrix[, 3],
      pValue = coef_matrix[, 4],
      row.names = NULL
    )
    significant_interactions <- subset(coef_df, grepl(":", Term) & pValue < 0.05)
    return(significant_interactions$Term)
}


best_interaction_formula <- function(additictive_predictors, df, target_var) {
  full_int_formula <- as.formula(paste(target_var, " ~ (", paste(coef_names, collapse = " + "), ")^2"))
  # cat ("current: ", deparse(full_int_formula), "\n")
  full_int_lm <- lm(full_int_formula, data = df)
  lm_summary <- summary(full_int_lm)
  repeat {
    significant_interactions_current <- find_significant_interactions(lm_summary$coefficients)
    n_significant_interactions_current <- length(significant_interactions_current)
    cat ("current: ", n_significant_interactions_current, "\n")
    # cat("new: ", deparse(significant_interactions_current), "\n")
    
    # Replace ":" with "*" in each element
    terms_modified <- gsub(":", "*", significant_interactions_current)
    # Concatenate all modified terms with "+"
    # interactions <- paste(terms_modified, collapse = " + ")
    int_formula_new <- as.formula(paste(target_var, " ~", paste(additictive_predictors, collapse = " + "), " + ", paste(terms_modified,collapse= "+")))
    # cat ("new: ", deparse(int_formula_new), "\n")
    int_formula_new_lm <- lm(int_formula_new, data = df)
    lm_summary <- summary(int_formula_new_lm)
    significant_interactions_new <- find_significant_interactions(lm_summary$coefficients)
    n_significant_interactions_new <- length(significant_interactions_new)
    cat ("new: ", n_significant_interactions_new, "\n")
    if (n_significant_interactions_new == n_significant_interactions_current) {
      return(int_formula_new)
    }
  }
}

best_int_formula <- best_interaction_formula(coef_names, pca_additive_final_df, "price")
best_int_lm <- lm(best_int_formula, data=pca_additive_final_df)
best_int_lm_summary <- summary(best_int_lm)
```

```{r}
best_int_formula <- best_interaction_formula(coef_names, pca_additive_final_df, "price")
best_int_lm <- lm(best_int_formula, data=pca_additive_final_df)
best_int_lm_summary <- summary(best_int_lm)
```




# Assumption check
## Linearity Assumption

The addictive model's $R^2_{adj} = `r lm_pca_significant_summary$adj.r.squared`$


```{r}
ggplot(best_int_lm, aes(x=.fitted, y=.resid)) +
geom_point(colour = "purple") +
geom_hline(yintercept = 0) +
geom_smooth(colour = "green4")+
ggtitle("Residual plot: Residual vs Fitted values")
```

There is no evident pattern, with occasional outlier.

## Independence Assumption

```{r}
# Plot residuals vs. observation order
plot(residuals(best_int_lm), type = "l", main = "Residuals vs. Observation Order",
     ylab = "Residual", xlab = "Observation Index")
abline(h = 0, col = "red")
```


The plot shows that follows a line but not a trend, indicating that the residuals are likely independent.

## Equal Variance Assumption

```{r}
ggplot(best_int_lm, aes(x=.fitted, y=.resid)) +
geom_point(colour = "purple") +
geom_hline(yintercept = 0) +
geom_smooth(colour = "green4")+
ggtitle("Residual plot: Residual vs Fitted values")
```

The green line shows that is overfitting the outliers, but there is no clear trends.

```{r}
library(lmtest)
bptest(best_int_lm)
```

Here we reject the null hypothesis (p-value < 0.05), so we conclude we do have heteroscedasticity. Maybe we can try a higher order model to get rid of this issue. Or log-transform the price

## Normality Assumption

```{r}
# histogram
ggplot(data=pca_significant_df, aes(best_int_lm$residuals)) +
geom_histogram(breaks = seq(-1,1,by=0.1), col="green3", fill="green4") +
labs(title="Histogram for residuals") +
labs(x="residuals", y="Count")
```


```{r}
ggplot(pca_significant_df, aes(sample=best_int_lm$residuals)) + 
  stat_qq() + 
  stat_qq_line()

par(mfrow=c(1,2))
```


```{r}
shapiro.test(residuals(best_int_lm))
```

Because p-value < alpha, we do reject null and fail to have normallity

# interaction 2

```{r}
library(MASS)
bc=boxcox(lm_pca,lambda=seq(-1,1))
bestlambda=bc$x[which(bc$y==max(bc$y))]
```


```{r}
bestlambda
```

The best lambda is `r bestlambda`.

```{r}
coef_table <- lm_pca_summary$coefficients
pca_predictors <- rownames(coef_table)
# 2. Remove the intercept manually (optional)
pca_predictors <- pca_predictors[pca_predictors != "(Intercept)"]
length(pca_predictors)
```

```{r}
df_pca$price_lambda <- (df_pca$price^bestlambda - 1)/bestlambda
```


```{r}
options(expressions = 20000)

pca_lambda_formula <- reformulate(termlabels = pca_predictors, response = "price_lambda")

# pca_lambda_formula
```


```{r}
memory.limit()  
```


```{r}

# Fit the new linear model using the significant predictors only
lm_pca_lambda <- lm(pca_lambda_formula, data = df_pca)
lm_pca_lambda_summary <- summary(lm_pca_lambda)
#lm_pca_significant_lambda_summary
```

```{r}
coef_table <- lm_pca_lambda_summary$coefficients
pca_lambda_significant_predictors <- rownames(coef_table)[coef_table[, 4] < 0.05]
length(pca_lambda_significant_predictors)
```
Since range of price is [`r range(df_pca$price)[1]`, `r range(df_pca$price)[2]`], it is safe to use box-cox transformation.


```{r}
options(expressions = 10000)  # to handle huge amount of predictors
pca_lambda_significant_formula <- as.formula(
  paste("((price^", bestlambda, " - 1)/", bestlambda, ") ~",
        paste(pca_lambda_significant_predictors[pca_lambda_significant_predictors != "(Intercept)"], collapse = " + "))
)
```

A new dataframe with only significant predictors is derived from pca's dataframe
```{r}
# Create a new data frame with only the significant predictors
# Assuming df contains the original dataset
pca_sign_no_intercept <- setdiff(pca_lambda_significant_predictors, "(Intercept)")

pca_significant_df <- df_pca[, pca_sign_no_intercept]

# Add the dependent variable (response) to the new dataframe if needed
pca_lambda_significant_df <- cbind(pca_significant_df, price=df_pca$price)  
```

```{r}
lm_pca_lambda_significant <- lm(pca_lambda_significant_formula, data = pca_lambda_significant_df)
lm_pca_lambda_significant_summary <- summary(lm_pca_lambda_significant)
```



```{r}
coef_table <- lm_pca_lambda_significant_summary$coefficients
recheck_significant_predictors <- rownames(coef_table)[coef_table[, 4] < 0.05]
length(recheck_significant_predictors)
```

On previous interaction, there where `r length(pca_lambda_significant_predictors)` statistically significant predictors, and now there are `r length(recheck_significant_predictors)`, so model will be refined again.


A new dataframe with only significant predictors is derived from pca's dataframe
```{r}
# Create a new data frame with only the significant predictors
# Assuming df contains the original dataset
pca_sign_no_intercept <- setdiff(recheck_significant_predictors, "(Intercept)")

pca_significant_df <- pca_lambda_significant_df[, pca_sign_no_intercept]

# Add the dependent variable (response) to the new dataframe if needed
pca_lambda_significant_df <- cbind(pca_lambda_significant_df, price=pca_lambda_significant_df$price)  
pca_lambda_significant_formula <- as.formula(
  paste("((price^", bestlambda, " - 1)/", bestlambda, ") ~",
        paste(recheck_significant_predictors[recheck_significant_predictors != "(Intercept)"], collapse = " + "))
)
lm_pca_lambda_significant <- lm(pca_lambda_significant_formula, data = pca_lambda_significant_df)
lm_pca_lambda_significant_summary <- summary(lm_pca_lambda_significant)
```

```{r}
coef_table <- lm_pca_lambda_significant_summary$coefficients
recheck_significant_predictors <- rownames(coef_table)[coef_table[, 4] < 0.05]
length(recheck_significant_predictors)
```


# Assumption check

## multi collinearity

```{r}
library(car)
options(max.print = 1000000000)
lm_pca_lambda_significant_vif <- vif(lm_pca_lambda_significant)
```

```{r}
vif_values <- as.data.frame(lm_pca_lambda_significant_vif)
filtered_vif <- vif_values[vif_values$VIF >= 5, ]
filtered_vif
```


## Linearity Assumption

The addictive model's $R^2_{adj} = `r lm_pca_lambda_significant_summary$adj.r.squared`$


```{r}
ggplot(lm_pca_lambda_significant, aes(x=.fitted, y=.resid)) +
geom_point(colour = "purple") +
geom_hline(yintercept = 0) +
geom_smooth(colour = "green4")+
ggtitle("Residual plot: Residual vs Fitted values")
```

There is no evident pattern, with occasional outlier.

## Independence Assumption

```{r}
# Plot residuals vs. observation order
plot(residuals(lm_pca_lambda_significant), type = "l", main = "Residuals vs. Observation Order",
     ylab = "Residual", xlab = "Observation Index")
abline(h = 0, col = "red")
```


The plot shows that follows a line but not a trend, indicating that the residuals are likely independent.

## Equal Variance Assumption

```{r}
ggplot(lm_pca_lambda_significant, aes(x=.fitted, y=.resid)) +
geom_point(colour = "purple") +
geom_hline(yintercept = 0) +
geom_smooth(colour = "green4")+
ggtitle("Residual plot: Residual vs Fitted values")
```

The green line shows that is overfitting the outliers, but there is no clear trends.

```{r}
library(lmtest)
bptest(lm_pca_lambda_significant)
```

Here we fail to reject the null hypothesis (p-value < 0.05), so we conclude we do have homoscedasticity.

## Normality Assumption

```{r}
# histogram
ggplot(data=pca_lambda_significant_df, aes(lm_pca_lambda_significant$residuals)) +
geom_histogram(breaks = seq(-1,1,by=0.1), col="green3", fill="green4") +
labs(title="Histogram for residuals") +
labs(x="residuals", y="Count")
```


```{r}
ggplot(pca_lambda_significant_df, aes(sample=lm_pca_lambda_significant$residuals)) + 
  stat_qq() + 
  stat_qq_line()

par(mfrow=c(1,2))
```


```{r}
shapiro.test(residuals(lm_pca_lambda_significant))
```

Because p-value < alpha, we do reject null and fail to have normallity



```{r}
pca_lambda_formula <- as.formula(
  paste("((price^",bestlambda,")-1/",bestlambda,") ~", 
        paste(pca_significant_predictors[pca_significant_predictors != "(Intercept)"], collapse = " + "))
)
# pca_lambda_formula
```

```{r}

# Fit the new linear model using the significant predictors only
lm_pca_significant_lambda <- lm(pca_lambda_formula, data = pca_significant_df)
lm_pca_significant_lambda_summary <- summary(lm_pca_significant_lambda)
# lm_pca_significant_lambda_summary
# bcmodel2=lm((((price^bestlambda)-1)/bestlambda)~stfees,data=University)
# summary(bcmodel2)
```





# CONCLUSION AND DISCUSSION
## Approach
*Overall, is the approach we took promising? Please elaborate. What different approach or variant of this approach is better?*

## Future work
*What should follow-up work be done next?*

# 5	REFERENCES 
*List all references (e.g., books, journal, conferences, reports, magazines) you are citing in this proposal. Format:*

1.	Data set. **Author(s)**: Taeef Najib, **Title**: Used Car Price Prediction Dataset, **Venue**: Kaggle, **Pages**: N/A, **Month/Year**: May 2022, **Publisher**: Kaggle, **URL**: https://www.kaggle.com/datasets/taeefnajib/used-car-price-prediction-dataset/data
2.	**Author(s)**: Sumeyra Muti , Kazım Yıldız, **Title**: Using Linear Regression For Used Car Price Prediction, **Venue**:International Journal of Computational and Experimental Science and Engineering (IJCESEN), **Pages**:137-141, **Month/Year**: 03/23, **Publisher**:Iğdır University, **URL**:https://doi.org/10.22399/ijcesen.1070505 . 
3.	(Your text here) Author(s), Title, Venue, Pages, Month/Year, Publisher.
4.	(Your text here) Author(s), Title, Venue, Pages, Month/Year, Publisher.




# APPENDIX
Optional part of the proposal. Include any support material you may have, e.g., figures, tables, samples of your data set(s). Make sure to cite any of the Appendix material in the main proposal text. 